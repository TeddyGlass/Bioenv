{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import (\n",
    "    roc_curve,\n",
    "    confusion_matrix,\n",
    "    accuracy_score,\n",
    "    matthews_corrcoef,\n",
    "    roc_auc_score,\n",
    ")\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from keras.models import model_from_json\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roc_cutoff(y_true, y_pred):\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_pred)\n",
    "    cutoff = thresholds[np.argmax(tpr - fpr)]\n",
    "    return cutoff\n",
    "\n",
    "\n",
    "def evaluate_clf(y_true, y_pred, cutoff):\n",
    "    pred_label = (y_pred >= cutoff) * 1\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, pred_label).ravel()\n",
    "    accuracy = accuracy_score(y_true, pred_label)\n",
    "    balanced_accuracy = (tp / (tp + fn) + tn / (tn + fp)) / 2\n",
    "    mcc = matthews_corrcoef(y_true, pred_label)\n",
    "    sensitivity = tp / (tp + fn)\n",
    "    specificity = tn / (tn + fp)\n",
    "    auc = roc_auc_score(y_true, y_pred)\n",
    "    metrics = {\n",
    "        'auc': [auc],\n",
    "        'acc': [accuracy],\n",
    "        'sen': [sensitivity],\n",
    "        'spe': [specificity],\n",
    "        'bac': [balanced_accuracy],\n",
    "        'mcc': [mcc],\n",
    "        'cutoff': [cutoff]\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def load_lgb(path):\n",
    "    if 'LGB' in path:\n",
    "        with open(path, 'rb') as f:\n",
    "            model = pickle.load(f)\n",
    "    return model\n",
    "\n",
    "\n",
    "def load_rf(path):\n",
    "    if 'RandomForest' in path:\n",
    "        with open(path, 'rb') as f:\n",
    "            model = pickle.load(f)\n",
    "    return model\n",
    "\n",
    "\n",
    "def load_svm(path):\n",
    "    if 'SVM' in path:\n",
    "        with open(path, 'rb') as f:\n",
    "            model = pickle.load(f)\n",
    "    return model\n",
    "\n",
    "\n",
    "def load_xgb(path):\n",
    "    if 'XGB' in path:\n",
    "        with open(path, 'rb') as f:\n",
    "            model = pickle.load(f)\n",
    "    return model\n",
    "\n",
    "\n",
    "def load_nn(path_json, path_h5, path_transformer):\n",
    "    if 'NN' in path_json:\n",
    "        with open(path_json, 'r') as f:\n",
    "            json_string = f.read()\n",
    "        model = model_from_json(json_string)\n",
    "        model.load_weights(path_h5)\n",
    "        if os.path.exists(path_transformer):\n",
    "            with open(path_transformer, 'rb') as f:\n",
    "                transformer = pickle.load(f)\n",
    "        else:\n",
    "            transformer = None\n",
    "        return model, transformer\n",
    "\n",
    "\n",
    "def sort_preds(PREDS, IDXES):\n",
    "    va_idxes = np.concatenate(IDXES)\n",
    "    order = np.argsort(va_idxes)\n",
    "    va_preds = np.concatenate(PREDS)\n",
    "    return va_preds[order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/kurosaki/Document/Research/PJ0/Repository/Bioenv/src\n"
     ]
    }
   ],
   "source": [
    "root = os.getcwd()\n",
    "print(root)\n",
    "\n",
    "n_splits_ncv = 5\n",
    "seed_ncv = 1712"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepPPIAAindex\n",
      "X_shape (65851, 1132)\n",
      "y_shape (65851,)\n",
      "****************************************************************************************************\n",
      "fold 0\n",
      "----------------------------------------------------------------------------------------------------\n",
      "LGBMClassifier\n",
      "       auc       acc       sen       spe       bac       mcc   cutoff\n",
      "0  0.97723  0.934629  0.924102  0.938368  0.931235  0.837877  0.23621\n",
      "----------------------------------------------------------------------------------------------------\n",
      "RandomForestClassifier\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=40)]: Using backend ThreadingBackend with 40 concurrent workers.\n",
      "[Parallel(n_jobs=40)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=40)]: Using backend ThreadingBackend with 40 concurrent workers.\n",
      "[Parallel(n_jobs=40)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=40)]: Using backend ThreadingBackend with 40 concurrent workers.\n",
      "[Parallel(n_jobs=40)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=40)]: Using backend ThreadingBackend with 40 concurrent workers.\n",
      "[Parallel(n_jobs=40)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=40)]: Using backend ThreadingBackend with 40 concurrent workers.\n",
      "[Parallel(n_jobs=40)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=40)]: Using backend ThreadingBackend with 40 concurrent workers.\n",
      "[Parallel(n_jobs=40)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=40)]: Using backend ThreadingBackend with 40 concurrent workers.\n",
      "[Parallel(n_jobs=40)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=40)]: Using backend ThreadingBackend with 40 concurrent workers.\n",
      "[Parallel(n_jobs=40)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=40)]: Using backend ThreadingBackend with 40 concurrent workers.\n",
      "[Parallel(n_jobs=40)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=40)]: Using backend ThreadingBackend with 40 concurrent workers.\n",
      "[Parallel(n_jobs=40)]: Done 100 out of 100 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       auc       acc       sen       spe       bac      mcc    cutoff\n",
      "0  0.91859  0.854681  0.856895  0.853894  0.855394  0.66356  0.197646\n",
      "----------------------------------------------------------------------------------------------------\n",
      "NNClassifier\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-06 14:29:16.023596: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-07-06 14:29:20.275730: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2022-07-06 14:29:20.276121: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2198655000 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        auc       acc       sen       spe      bac       mcc   cutoff\n",
      "0  0.882535  0.811252  0.791425  0.818294  0.80486  0.565434  0.27198\n",
      "****************************************************************************************************\n",
      "DeepPPIAutocorr\n",
      "X_shape (65851, 1440)\n",
      "y_shape (65851,)\n",
      "****************************************************************************************************\n",
      "fold 0\n",
      "----------------------------------------------------------------------------------------------------\n",
      "LGBMClassifier\n",
      "        auc       acc       sen       spe       bac       mcc    cutoff\n",
      "0  0.977725  0.931592  0.926999  0.933224  0.930111  0.831818  0.224188\n",
      "----------------------------------------------------------------------------------------------------\n",
      "RandomForestClassifier\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=40)]: Using backend ThreadingBackend with 40 concurrent workers.\n",
      "[Parallel(n_jobs=40)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=40)]: Using backend ThreadingBackend with 40 concurrent workers.\n",
      "[Parallel(n_jobs=40)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=40)]: Using backend ThreadingBackend with 40 concurrent workers.\n",
      "[Parallel(n_jobs=40)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=40)]: Using backend ThreadingBackend with 40 concurrent workers.\n",
      "[Parallel(n_jobs=40)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=40)]: Using backend ThreadingBackend with 40 concurrent workers.\n",
      "[Parallel(n_jobs=40)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=40)]: Using backend ThreadingBackend with 40 concurrent workers.\n",
      "[Parallel(n_jobs=40)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=40)]: Using backend ThreadingBackend with 40 concurrent workers.\n",
      "[Parallel(n_jobs=40)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=40)]: Using backend ThreadingBackend with 40 concurrent workers.\n",
      "[Parallel(n_jobs=40)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=40)]: Using backend ThreadingBackend with 40 concurrent workers.\n",
      "[Parallel(n_jobs=40)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=40)]: Using backend ThreadingBackend with 40 concurrent workers.\n",
      "[Parallel(n_jobs=40)]: Done 100 out of 100 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        auc       acc       sen       spe       bac       mcc    cutoff\n",
      "0  0.802667  0.768962  0.693801  0.795658  0.744729  0.456858  0.280524\n",
      "----------------------------------------------------------------------------------------------------\n",
      "NNClassifier\n",
      "        auc       acc     sen       spe       bac       mcc    cutoff\n",
      "0  0.941129  0.877838  0.8708  0.880337  0.875569  0.710199  0.211393\n",
      "****************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "f_names = [\n",
    "        'DeepPPIDescriptorAAindex.csv',\n",
    "        'DeepPPIDescriptorAutocorrelation.csv'\n",
    "    ]\n",
    "data_names = [\n",
    "    'DeepPPIAAindex',\n",
    "    'DeepPPIAutocorr'\n",
    "    ]\n",
    "\n",
    "results = {\n",
    "    data:{\n",
    "        i:{\n",
    "        'LGBMClassifier':None,\n",
    "        'RandomForestClassifier':None,\n",
    "        'NNClassifier':None\n",
    "        }\n",
    "        for i in range(5)\n",
    "        }\n",
    "        for data in data_names\n",
    "}\n",
    "\n",
    "for f_name, data_name in zip(f_names, data_names):\n",
    "     # load core data set for PPI classification\n",
    "    df = pd.read_csv('../data/DeepPPI/DeepPPIAll.csv')\n",
    "    protein_a = np.array(df['proteinA'])\n",
    "    protein_b = np.array(df['proteinB'])\n",
    "    y = np.array(df['interaction'])\n",
    "\n",
    "    # load features\n",
    "    df_feature = pd.read_csv(f'../data/DeepPPI/{f_name}')\n",
    "    # pre-processing of protein features\n",
    "    feature_dict = {\n",
    "        Id: np.array(df_feature[df_feature.iloc[:,0]==Id].iloc[:,1:])\n",
    "        for Id in df_feature.iloc[:,0].tolist()\n",
    "    }\n",
    "    feature_a, feature_b = [], []\n",
    "    for a, b in zip(protein_a, protein_b):\n",
    "        feature_a.append(feature_dict[a])\n",
    "        feature_b.append(feature_dict[b])\n",
    "    X_a, X_b = np.concatenate(feature_a), np.concatenate(feature_b)\n",
    "    X = np.concatenate([X_a, X_b], axis=1)\n",
    "    print(data_name)\n",
    "    print('X_shape', X.shape)\n",
    "    print('y_shape', y.shape)\n",
    "    skf_outer = StratifiedKFold(n_splits=n_splits_ncv, random_state=seed_ncv, shuffle=True) \n",
    "    outer_idxes = list(skf_outer.split(X, y))\n",
    "\n",
    "    # evaluate ptrained model\n",
    "    for i, (inner_idx, te_idx) in enumerate(outer_idxes):\n",
    "        if i == 0:\n",
    "            print('*'*100)\n",
    "            print('fold', i)\n",
    "            for model_name in ['LGBMClassifier', 'RandomForestClassifier', 'NNClassifier']:\n",
    "                skf_inner = StratifiedKFold(n_splits=n_splits_ncv, random_state=seed_ncv, shuffle=True)\n",
    "                inner_idxes = list(skf_outer.split(X[inner_idx], y[inner_idx]))\n",
    "                print('-'*100)\n",
    "                print(model_name)\n",
    "                te_preds, cutoffs = [], []\n",
    "                for j, (tr_idx, va_idx) in enumerate(inner_idxes):\n",
    "                    if 'LGB' in model_name:\n",
    "                        model = load_lgb(f'../results/models/{data_name}/LGBMClassifier/LGBMClassifier_ij{i}{j}_trainedmodel.pkl')\n",
    "                        va_pred = model.predict_proba(X[inner_idx][va_idx], num_iterations=model.best_iteration_)[:,1]\n",
    "                        te_pred = model.predict_proba(X[te_idx], num_iterations=model.best_iteration_)[:,1]\n",
    "                    elif 'RandomForest' in model_name:\n",
    "                        model = load_rf(f'../results/models/{data_name}/RandomForestClassifier/RandomForestClassifier_ij{i}{j}_trainedmodel.pkl')\n",
    "                        va_pred = model.predict_proba(X[inner_idx][va_idx])[:,1]\n",
    "                        te_pred = model.predict_proba(X[te_idx])[:,1]\n",
    "                    elif 'NN' in model_name:\n",
    "                        model, transformer = load_nn(\n",
    "                            f'../results/models/{data_name}/NNClassifier/NNClassifier_i{i}_architecture.json',\n",
    "                            f'../results/models/{data_name}/NNClassifier/NNClassifier_ij{i}{j}_trainedweight.h5',\n",
    "                            f'../results/models/{data_name}/NNClassifier/NNClassifier_ij{i}{j}_transformer.pkl'\n",
    "                            )\n",
    "                        va_pred = model.predict(transformer.transform(X[inner_idx][va_idx]))\n",
    "                        te_pred = model.predict(transformer.transform(X[te_idx]))\n",
    "                cutoff = roc_cutoff(y[inner_idx][va_idx], va_pred)\n",
    "                te_preds.append(te_pred)\n",
    "                cutoffs.append(cutoff)\n",
    "                te_pred_mean = np.mean(te_preds, axis=0)\n",
    "                metrics = evaluate_clf(y[te_idx], te_pred_mean, np.mean(cutoffs))\n",
    "                print(pd.DataFrame(metrics))\n",
    "                results[data_name][i][model_name] = pd.DataFrame(metrics)\n",
    "            print('*'*100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = []\n",
    "for data_name in data_names:\n",
    "    result_dict = results[data_name][0]\n",
    "    df_result = pd.concat([v for v in result_dict.values()])\n",
    "    df_result['data'] = data_name\n",
    "    df_result.index = list(result_dict.keys())[-len(df_result):]\n",
    "    df_results.append(df_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>auc</th>\n",
       "      <th>acc</th>\n",
       "      <th>sen</th>\n",
       "      <th>spe</th>\n",
       "      <th>bac</th>\n",
       "      <th>mcc</th>\n",
       "      <th>cutoff</th>\n",
       "      <th>data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LGBMClassifier</th>\n",
       "      <td>0.977230</td>\n",
       "      <td>0.934629</td>\n",
       "      <td>0.924102</td>\n",
       "      <td>0.938368</td>\n",
       "      <td>0.931235</td>\n",
       "      <td>0.837877</td>\n",
       "      <td>0.236210</td>\n",
       "      <td>DeepPPIAAindex</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandomForestClassifier</th>\n",
       "      <td>0.918590</td>\n",
       "      <td>0.854681</td>\n",
       "      <td>0.856895</td>\n",
       "      <td>0.853894</td>\n",
       "      <td>0.855394</td>\n",
       "      <td>0.663560</td>\n",
       "      <td>0.197646</td>\n",
       "      <td>DeepPPIAAindex</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NNClassifier</th>\n",
       "      <td>0.882535</td>\n",
       "      <td>0.811252</td>\n",
       "      <td>0.791425</td>\n",
       "      <td>0.818294</td>\n",
       "      <td>0.804860</td>\n",
       "      <td>0.565434</td>\n",
       "      <td>0.271980</td>\n",
       "      <td>DeepPPIAAindex</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LGBMClassifier</th>\n",
       "      <td>0.977725</td>\n",
       "      <td>0.931592</td>\n",
       "      <td>0.926999</td>\n",
       "      <td>0.933224</td>\n",
       "      <td>0.930111</td>\n",
       "      <td>0.831818</td>\n",
       "      <td>0.224188</td>\n",
       "      <td>DeepPPIAutocorr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandomForestClassifier</th>\n",
       "      <td>0.802667</td>\n",
       "      <td>0.768962</td>\n",
       "      <td>0.693801</td>\n",
       "      <td>0.795658</td>\n",
       "      <td>0.744729</td>\n",
       "      <td>0.456858</td>\n",
       "      <td>0.280524</td>\n",
       "      <td>DeepPPIAutocorr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NNClassifier</th>\n",
       "      <td>0.941129</td>\n",
       "      <td>0.877838</td>\n",
       "      <td>0.870800</td>\n",
       "      <td>0.880337</td>\n",
       "      <td>0.875569</td>\n",
       "      <td>0.710199</td>\n",
       "      <td>0.211393</td>\n",
       "      <td>DeepPPIAutocorr</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             auc       acc       sen       spe       bac  \\\n",
       "LGBMClassifier          0.977230  0.934629  0.924102  0.938368  0.931235   \n",
       "RandomForestClassifier  0.918590  0.854681  0.856895  0.853894  0.855394   \n",
       "NNClassifier            0.882535  0.811252  0.791425  0.818294  0.804860   \n",
       "LGBMClassifier          0.977725  0.931592  0.926999  0.933224  0.930111   \n",
       "RandomForestClassifier  0.802667  0.768962  0.693801  0.795658  0.744729   \n",
       "NNClassifier            0.941129  0.877838  0.870800  0.880337  0.875569   \n",
       "\n",
       "                             mcc    cutoff             data  \n",
       "LGBMClassifier          0.837877  0.236210   DeepPPIAAindex  \n",
       "RandomForestClassifier  0.663560  0.197646   DeepPPIAAindex  \n",
       "NNClassifier            0.565434  0.271980   DeepPPIAAindex  \n",
       "LGBMClassifier          0.831818  0.224188  DeepPPIAutocorr  \n",
       "RandomForestClassifier  0.456858  0.280524  DeepPPIAutocorr  \n",
       "NNClassifier            0.710199  0.211393  DeepPPIAutocorr  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_summary = pd.concat(df_results)\n",
    "df_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_summary.to_csv('../results/summary/DSCDeepPPI.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3e12de8d4f511a3f40a134d20c84f219b709a35adcae8dcc2b376a736afcf2a"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('bioenv_ver0.1')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
